2022-08-03 13:13:55.086795: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Params: {'model_checkpoint': 'xlm-roberta-large', 'source_column': 'gloss', 'max_len': 150, 'batch_size': 16, 'dropout': 0.1, 'learning_rate': 0.0001, 'num_epochs': 150, 'early_stopping_limit': 5, 'device': 'cuda:0', 'loss_fn_name': 'cosine', 'emb_type': 'fasttext', 'use_adapters': False, 'resume_from_checkpoint': False, 'output_size': 300, 'knn_measure': 'cosine', 'num_warmup_steps': 300, 'extra_data': False}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31444/31444 [00:01<00:00, 31196.33it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4593/4593 [00:00<00:00, 45285.47it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4529/4529 [00:00<00:00, 43551.33it/s]
Len of Train before drop duplicates - 28983
Len of Val before drop duplicates - 4219
Len of Test before drop duplicates - 4227
Len of Train - 28983
Len of Val - 4219
Len of Test - 4227
fasttext
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 28983 entries, 0 to 28982
Data columns (total 3 columns):
 #   Column    Non-Null Count  Dtype
---  ------    --------------  -----
 0   word      28983 non-null  object
 1   gloss     28983 non-null  object
 2   fasttext  28983 non-null  object
dtypes: object(3)
memory usage: 679.4+ KB
None
              word                                              gloss                                           fasttext
27154  underground  A movement or organisation of people who resis...  [-0.2013, -0.3014, 0.1525, 0.1562, -0.304, -0....
20981    refection  Physical refreshment , especially with food or...  [0.0731, 0.2905, -0.5642, -0.0788, -0.0255, 0....
22957      shelter                                    To take cover .  [-0.0753, -0.5014, -0.2117, -0.1908, -0.3724, ...
13664   indecisive                          inconclusive or uncertain  [-0.0403, 0.0665, 0.1213, -0.7659, 0.5997, 0.0...
2929       bedevil       To harass or cause trouble for ; to plague .  [0.1295, -0.0428, 0.0991, -0.1387, 0.3032, 0.1...
24343        stage          To place in position to prepare for use .  [0.0975, -0.3233, -0.1921, -0.0952, 0.0622, 0....

Loading Tokenizer: xlm-roberta-large ...
Save checkpoint path: ../checkpoints/xlm-roberta-large_12
Train len 1812, val len: 264

Using device cuda:0 for training...
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model parameters count: 307500

Creating optimizer with Learning Rate: 0.0001

Training for 150 epochs with early stopping set to 5


num_training steps : 271800
  0%|                                                                                                                                                                                      | 0/271 [00:00<?, ?it/s][Epoch 0 / 150]
  0%|▌                                                                                                                                                               | 1/271 [00:33<2:29:46, 33.28s/it, loss=0.744]
Epoch: 0 	Training Loss: 0.695090 	Validation Loss: 0.627271
Loss improved saving checkpoint...
[Epoch 1 / 150]
  1%|█▊                                                                                                                                                               | 3/271 [01:43<2:31:49, 33.99s/it, loss=0.61]
Epoch: 1 	Training Loss: 0.603892 	Validation Loss: 0.579463
Loss improved saving checkpoint...
[Epoch 2 / 150]
  2%|██▉                                                                                                                                                             | 5/271 [03:08<2:49:50, 38.31s/it, loss=0.579]
Epoch: 2 	Training Loss: 0.577039 	Validation Loss: 0.563403
Loss improved saving checkpoint...
[Epoch 3 / 150]
  3%|████▏                                                                                                                                                           | 7/271 [04:26<2:46:14, 37.78s/it, loss=0.566]
Epoch: 3 	Training Loss: 0.566429 	Validation Loss: 0.556420
Loss improved saving checkpoint...
[Epoch 4 / 150]
  3%|█████▎                                                                                                                                                           | 9/271 [05:49<2:49:27, 38.81s/it, loss=0.56]
Epoch: 4 	Training Loss: 0.560486 	Validation Loss: 0.551278
Loss improved saving checkpoint...
[Epoch 5 / 150]
  4%|█████▊                                                                                                                                                         | 10/271 [06:34<2:57:32, 40.81s/it, loss=0.557]
Epoch: 5 	Training Loss: 0.556663 	Validation Loss: 0.547480
Loss improved saving checkpoint...
[Epoch 6 / 150]
  4%|███████                                                                                                                                                        | 12/271 [07:52<2:49:21, 39.23s/it, loss=0.554]
Epoch: 6 	Training Loss: 0.553839 	Validation Loss: 0.544929
Loss improved saving checkpoint...
[Epoch 7 / 150]
  5%|████████▏                                                                                                                                                      | 14/271 [09:15<2:51:12, 39.97s/it, loss=0.551]
Epoch: 7 	Training Loss: 0.551337 	Validation Loss: 0.543867
Loss improved saving checkpoint...
[Epoch 8 / 150]
  6%|█████████▍                                                                                                                                                      | 16/271 [10:31<2:43:15, 38.41s/it, loss=0.55]
Epoch: 8 	Training Loss: 0.550064 	Validation Loss: 0.541898
Loss improved saving checkpoint...
[Epoch 9 / 150]
  7%|██████████▌                                                                                                                                                    | 18/271 [11:51<2:43:08, 38.69s/it, loss=0.549]
Epoch: 9 	Training Loss: 0.548770 	Validation Loss: 0.540312
Loss improved saving checkpoint...
[Epoch 10 / 150]
  7%|███████████▏                                                                                                                                                   | 19/271 [12:38<2:52:51, 41.16s/it, loss=0.546]
Epoch: 10 	Training Loss: 0.547593 	Validation Loss: 0.539015
Loss improved saving checkpoint...
[Epoch 11 / 150]
  8%|████████████▎                                                                                                                                                  | 21/271 [13:57<2:44:39, 39.52s/it, loss=0.546]
Epoch: 11 	Training Loss: 0.546634 	Validation Loss: 0.538697
Loss improved saving checkpoint...
[Epoch 12 / 150]
  8%|█████████████▍                                                                                                                                                 | 23/271 [15:14<2:38:27, 38.34s/it, loss=0.546]
Epoch: 12 	Training Loss: 0.545804 	Validation Loss: 0.539165
[Epoch 13 / 150]
  9%|██████████████▋                                                                                                                                                | 25/271 [16:25<2:30:14, 36.65s/it, loss=0.545]
Epoch: 13 	Training Loss: 0.545318 	Validation Loss: 0.537424
Loss improved saving checkpoint...
[Epoch 14 / 150]
 10%|███████████████▊                                                                                                                                               | 27/271 [17:52<2:38:31, 38.98s/it, loss=0.545]
Epoch: 14 	Training Loss: 0.545274 	Validation Loss: 0.537103
Loss improved saving checkpoint...
[Epoch 15 / 150]
 10%|████████████████▍                                                                                                                                              | 28/271 [18:46<2:55:33, 43.35s/it, loss=0.544]
Epoch: 15 	Training Loss: 0.544567 	Validation Loss: 0.536449
Loss improved saving checkpoint...
[Epoch 16 / 150]
 11%|█████████████████▌                                                                                                                                             | 30/271 [20:01<2:39:39, 39.75s/it, loss=0.543]
Epoch: 16 	Training Loss: 0.543887 	Validation Loss: 0.536274
Loss improved saving checkpoint...
[Epoch 17 / 150]
 12%|██████████████████▊                                                                                                                                            | 32/271 [21:15<2:30:37, 37.81s/it, loss=0.544]
Epoch: 17 	Training Loss: 0.543490 	Validation Loss: 0.535199
Loss improved saving checkpoint...
[Epoch 18 / 150]
 13%|███████████████████▉                                                                                                                                           | 34/271 [22:35<2:31:26, 38.34s/it, loss=0.544]
Epoch: 18 	Training Loss: 0.543639 	Validation Loss: 0.536078
[Epoch 19 / 150]
 13%|█████████████████████                                                                                                                                          | 36/271 [23:49<2:26:04, 37.30s/it, loss=0.543]
Epoch: 19 	Training Loss: 0.543053 	Validation Loss: 0.536265
[Epoch 20 / 150]
 14%|██████████████████████▎                                                                                                                                        | 38/271 [25:00<2:20:04, 36.07s/it, loss=0.543]
Epoch: 20 	Training Loss: 0.542959 	Validation Loss: 0.534821
Loss improved saving checkpoint...
[Epoch 21 / 150]
 14%|██████████████████████▉                                                                                                                                        | 39/271 [25:44<2:27:58, 38.27s/it, loss=0.542]
Epoch: 21 	Training Loss: 0.542821 	Validation Loss: 0.534870
[Epoch 22 / 150]
 15%|████████████████████████                                                                                                                                       | 41/271 [26:52<2:17:12, 35.79s/it, loss=0.543]
Epoch: 22 	Training Loss: 0.542668 	Validation Loss: 0.534634
Loss improved saving checkpoint...
[Epoch 23 / 150]
 16%|█████████████████████████▏                                                                                                                                     | 43/271 [28:10<2:20:20, 36.93s/it, loss=0.542]
Epoch: 23 	Training Loss: 0.542225 	Validation Loss: 0.534124
Loss improved saving checkpoint...
[Epoch 24 / 150]
 17%|██████████████████████████▍                                                                                                                                    | 45/271 [29:24<2:17:13, 36.43s/it, loss=0.542]
Epoch: 24 	Training Loss: 0.542119 	Validation Loss: 0.534134
[Epoch 25 / 150]
 17%|███████████████████████████▌                                                                                                                                   | 47/271 [30:32<2:10:03, 34.84s/it, loss=0.542]
Epoch: 25 	Training Loss: 0.541854 	Validation Loss: 0.534027
Loss improved saving checkpoint...
[Epoch 26 / 150]
 18%|████████████████████████████▎                                                                                                                                   | 48/271 [31:16<2:18:56, 37.38s/it, loss=0.54]
Epoch: 26 	Training Loss: 0.541740 	Validation Loss: 0.533612
Loss improved saving checkpoint...
[Epoch 27 / 150]
 18%|█████████████████████████████▎                                                                                                                                 | 50/271 [32:29<2:14:28, 36.51s/it, loss=0.541]
Epoch: 27 	Training Loss: 0.542011 	Validation Loss: 0.533829
[Epoch 28 / 150]
 19%|██████████████████████████████▌                                                                                                                                | 52/271 [33:37<2:07:34, 34.95s/it, loss=0.541]
Epoch: 28 	Training Loss: 0.541353 	Validation Loss: 0.533438
Loss improved saving checkpoint...
[Epoch 29 / 150]
 20%|███████████████████████████████▋                                                                                                                               | 54/271 [34:51<2:07:48, 35.34s/it, loss=0.541]
Epoch: 29 	Training Loss: 0.541571 	Validation Loss: 0.533335
Loss improved saving checkpoint...
[Epoch 30 / 150]
 21%|████████████████████████████████▊                                                                                                                              | 56/271 [36:05<2:07:07, 35.48s/it, loss=0.541]
Epoch: 30 	Training Loss: 0.541306 	Validation Loss: 0.533394
[Epoch 31 / 150]
 21%|█████████████████████████████████▍                                                                                                                             | 57/271 [36:42<2:08:57, 36.16s/it, loss=0.541]
Epoch: 31 	Training Loss: 0.541100 	Validation Loss: 0.533090
Loss improved saving checkpoint...
[Epoch 32 / 150]
 22%|██████████████████████████████████▊                                                                                                                             | 59/271 [37:53<2:05:00, 35.38s/it, loss=0.54]
Epoch: 32 	Training Loss: 0.541057 	Validation Loss: 0.533174
[Epoch 33 / 150]
 23%|████████████████████████████████████                                                                                                                            | 61/271 [39:01<1:59:46, 34.22s/it, loss=0.54]
Epoch: 33 	Training Loss: 0.540572 	Validation Loss: 0.532013
Loss improved saving checkpoint...
[Epoch 34 / 150]
 23%|█████████████████████████████████████▏                                                                                                                          | 63/271 [40:14<2:01:07, 34.94s/it, loss=0.54]
Epoch: 34 	Training Loss: 0.540852 	Validation Loss: 0.532057
[Epoch 35 / 150]
 24%|██████████████████████████████████████▏                                                                                                                        | 65/271 [41:22<1:56:52, 34.04s/it, loss=0.541]
Epoch: 35 	Training Loss: 0.540710 	Validation Loss: 0.533293
[Epoch 36 / 150]
 25%|███████████████████████████████████████▎                                                                                                                       | 67/271 [42:29<1:54:05, 33.56s/it, loss=0.541]
Epoch: 36 	Training Loss: 0.540689 	Validation Loss: 0.531483
Loss improved saving checkpoint...
[Epoch 37 / 150]
 25%|████████████████████████████████████████▏                                                                                                                       | 68/271 [43:13<2:03:15, 36.43s/it, loss=0.54]
Epoch: 37 	Training Loss: 0.540721 	Validation Loss: 0.532722
[Epoch 38 / 150]
 25%|████████████████████████████████████████▍                                                                                                                      | 69/271 [43:50<2:03:39, 36.73s/it, loss=0.542]
 26%|█████████████████████████████████████████▎                                                                                                                      | 70/271 [44:20<1:56:23, 34.74s/it, loss=0.54]
Epoch: 38 	Training Loss: 0.540354 	Validation Loss: 0.531075
Loss improved saving checkpoint...
[Epoch 39 / 150]
 27%|██████████████████████████████████████████▌                                                                                                                     | 72/271 [45:34<1:56:58, 35.27s/it, loss=0.54]
Epoch: 39 	Training Loss: 0.540502 	Validation Loss: 0.532090
[Epoch 40 / 150]
 27%|███████████████████████████████████████████▋                                                                                                                    | 74/271 [46:41<1:52:08, 34.15s/it, loss=0.54]
Epoch: 40 	Training Loss: 0.540362 	Validation Loss: 0.532164
[Epoch 41 / 150]
 28%|████████████████████████████████████████████▊                                                                                                                   | 76/271 [48:09<2:08:58, 39.68s/it, loss=0.54]
Epoch: 41 	Training Loss: 0.540030 	Validation Loss: 0.532145
[Epoch 42 / 150]
 28%|█████████████████████████████████████████████▏                                                                                                                 | 77/271 [49:14<2:32:28, 47.16s/it, loss=0.539]
Epoch: 42 	Training Loss: 0.540324 	Validation Loss: 0.531691
[Epoch 43 / 150]
 29%|██████████████████████████████████████████████▋                                                                                                                 | 79/271 [51:11<2:47:58, 52.49s/it, loss=0.54]
Epoch: 43 	Training Loss: 0.540184 	Validation Loss: 0.531073
Loss improved saving checkpoint...
[Epoch 44 / 150]
 30%|███████████████████████████████████████████████▌                                                                                                               | 81/271 [53:14<2:57:36, 56.08s/it, loss=0.541]
Epoch: 44 	Training Loss: 0.540052 	Validation Loss: 0.531517
[Epoch 45 / 150]
 31%|█████████████████████████████████████████████████                                                                                                               | 83/271 [55:12<2:58:17, 56.90s/it, loss=0.54]
Epoch: 45 	Training Loss: 0.540267 	Validation Loss: 0.531695
[Epoch 46 / 150]
 31%|██████████████████████████████████████████████████▏                                                                                                             | 85/271 [57:08<2:56:51, 57.05s/it, loss=0.54]
Epoch: 46 	Training Loss: 0.540208 	Validation Loss: 0.530787
Loss improved saving checkpoint...
[Epoch 47 / 150]
 32%|██████████████████████████████████████████████████▍                                                                                                            | 86/271 [58:16<3:06:08, 60.37s/it, loss=0.539]
Epoch: 47 	Training Loss: 0.539949 	Validation Loss: 0.530549
Loss improved saving checkpoint...
[Epoch 48 / 150]
 32%|██████████████████████████████████████████████████▉                                                                                                          | 88/271 [1:00:21<3:04:22, 60.45s/it, loss=0.539]
Epoch: 48 	Training Loss: 0.539813 	Validation Loss: 0.531181
[Epoch 49 / 150]
 33%|████████████████████████████████████████████████████▏                                                                                                        | 90/271 [1:02:21<2:59:56, 59.65s/it, loss=0.539]
Epoch: 49 	Training Loss: 0.539829 	Validation Loss: 0.530837
[Epoch 50 / 150]
 34%|█████████████████████████████████████████████████████▎                                                                                                       | 92/271 [1:05:18<3:40:05, 73.77s/it, loss=0.539]
Epoch: 50 	Training Loss: 0.539444 	Validation Loss: 0.530763
[Epoch 51 / 150]
 35%|██████████████████████████████████████████████████████▍                                                                                                      | 94/271 [1:08:18<3:59:11, 81.08s/it, loss=0.539]
Epoch: 51 	Training Loss: 0.539335 	Validation Loss: 0.531480
[Epoch 52 / 150]
 35%|███████████████████████████████████████████████████████▌                                                                                                     | 96/271 [1:11:19<4:07:50, 84.98s/it, loss=0.539]
Epoch: 52 	Training Loss: 0.539502 	Validation Loss: 0.530672

Loss did not reduce for last 5 epochs. Stopped training..
 35%|███████████████████████████████████████████████████████▌                                                                                                     | 96/271 [1:11:43<2:10:44, 44.83s/it, loss=0.539]