2022-08-05 21:01:52.768906: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Params: {'model_checkpoint': 'xlm-roberta-large', 'source_column': 'gloss', 'max_len': 150, 'batch_size': 16, 'dropout': 0.1, 'learning_rate': 0.0001, 'num_epochs': 150, 'early_stopping_limit': 5, 'device': 'cuda:0', 'loss_fn_name': 'cosine', 'emb_type': 'bert-base-uncased', 'use_adapters': True, 'resume_from_checkpoint': False, 'output_size': 768, 'knn_measure': 'cosine', 'num_warmup_steps': 300, 'extra_data': False}
768 <class 'int'>
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31444/31444 [00:01<00:00, 21113.80it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4593/4593 [00:00<00:00, 21088.09it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4529/4529 [00:00<00:00, 20996.51it/s]
Len of Train before drop duplicates - 30355
Len of Val before drop duplicates - 4535
Len of Test before drop duplicates - 4442
Len of Train - 30355
Len of Val - 4535
Len of Test - 4442
bert-base-uncased
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 30355 entries, 0 to 30354
Data columns (total 3 columns):
 #   Column             Non-Null Count  Dtype
---  ------             --------------  -----
 0   word               30355 non-null  object
 1   gloss              30355 non-null  object
 2   bert-base-uncased  30355 non-null  object
dtypes: object(3)
memory usage: 711.6+ KB
None
                word                                              gloss                                  bert-base-uncased
18729     one-to-one  Matching each member of one set with exactly o...  [-0.7049232959747315, -0.41651648879051206, 0....
26784     tax-exempt  Exempt from paying tax ; not subject to taxati...  [-0.13599122812350592, -0.5020736828446388, 0....
28223  unaccountable  not responsible ; free from accountability or ...  [-0.40933319367468357, -0.6113631594926119, 0....
210          Clemson  To uncharacteristically poorly , well below ex...  [-0.17428807727992535, -0.5479303374886513, -0...
19408    paperweight  A small , decorative , somewhat weighty object...  [0.31155911087989807, 0.21598078310489655, 0.1...
16302       lordship  Title applied to a lord , bishop , judge , or ...  [-0.30310140550136566, 0.4470379501581192, 0.4...

Loading Tokenizer: xlm-roberta-large ...
Save checkpoint path: ../checkpoints/xlm-roberta-large_bert-base-uncased_0
Train len 1898, val len: 284

Using device cuda:0 for training...
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model parameters count: 3166080

Creating optimizer with Learning Rate: 0.0001

Training for 150 epochs with early stopping set to 5


num_training steps : 284700
  0%|                                                                                                                                                                                                    | 0/284 [00:00<?, ?it/s][Epoch 0 / 150]
  0%|▌                                                                                                                                                                           | 1/284 [02:20<11:01:13, 140.19s/it, loss=0.488]
Epoch: 0 	Training Loss: 0.454230 	Validation Loss: 0.400712
Loss improved saving checkpoint...
[Epoch 1 / 150]
  1%|█▊                                                                                                                                                                          | 3/284 [07:12<11:14:35, 144.04s/it, loss=0.391]
Epoch: 1 	Training Loss: 0.384904 	Validation Loss: 0.372088
Loss improved saving checkpoint...
[Epoch 2 / 150]
  2%|███                                                                                                                                                                          | 5/284 [11:03<9:29:58, 122.58s/it, loss=0.367]
Epoch: 2 	Training Loss: 0.365565 	Validation Loss: 0.361893
Loss improved saving checkpoint...
[Epoch 3 / 150]
  2%|████▎                                                                                                                                                                        | 7/284 [15:29<9:47:15, 127.20s/it, loss=0.357]
Epoch: 3 	Training Loss: 0.356742 	Validation Loss: 0.356682
Loss improved saving checkpoint...
[Epoch 4 / 150]
  3%|█████▌                                                                                                                                                                        | 9/284 [19:56<9:55:18, 129.88s/it, loss=0.35]
Epoch: 4 	Training Loss: 0.349449 	Validation Loss: 0.353344
Loss improved saving checkpoint...
[Epoch 5 / 150]
  4%|██████▋                                                                                                                                                                     | 11/284 [24:25<9:59:57, 131.86s/it, loss=0.344]
Epoch: 5 	Training Loss: 0.344268 	Validation Loss: 0.351576
Loss improved saving checkpoint...
[Epoch 6 / 150]
  5%|███████▊                                                                                                                                                                    | 13/284 [28:59<10:03:54, 133.71s/it, loss=0.34]
Epoch: 6 	Training Loss: 0.339857 	Validation Loss: 0.352340
[Epoch 7 / 150]
  5%|█████████                                                                                                                                                                   | 15/284 [33:26<9:56:36, 133.07s/it, loss=0.336]
Epoch: 7 	Training Loss: 0.335678 	Validation Loss: 0.351096
Loss improved saving checkpoint...
[Epoch 8 / 150]
  6%|██████████▎                                                                                                                                                                 | 17/284 [37:57<9:52:51, 133.23s/it, loss=0.332]
Epoch: 8 	Training Loss: 0.331678 	Validation Loss: 0.350430
Loss improved saving checkpoint...
[Epoch 9 / 150]
  6%|██████████▊                                                                                                                                                                | 18/284 [40:19<10:02:57, 136.01s/it, loss=0.328]
Epoch: 9 	Training Loss: 0.327953 	Validation Loss: 0.351181
[Epoch 10 / 150]
  7%|████████████                                                                                                                                                                | 20/284 [44:46<9:51:17, 134.38s/it, loss=0.324]
Epoch: 10 	Training Loss: 0.324543 	Validation Loss: 0.350958
[Epoch 11 / 150]
  8%|█████████████▎                                                                                                                                                              | 22/284 [48:30<8:51:52, 121.80s/it, loss=0.321]
Epoch: 11 	Training Loss: 0.320989 	Validation Loss: 0.351588
[Epoch 12 / 150]
  8%|██████████████▌                                                                                                                                                             | 24/284 [52:09<8:18:49, 115.11s/it, loss=0.317]
Epoch: 12 	Training Loss: 0.317682 	Validation Loss: 0.352589
[Epoch 13 / 150]
  9%|███████████████▋                                                                                                                                                            | 26/284 [55:49<8:00:03, 111.64s/it, loss=0.314]
Epoch: 13 	Training Loss: 0.314495 	Validation Loss: 0.352518

Loss did not reduce for last 5 epochs. Stopped training..
  9%|███████████████▋                                                                                                                                                            | 26/284 [57:02<9:26:03, 131.64s/it, loss=0.314]