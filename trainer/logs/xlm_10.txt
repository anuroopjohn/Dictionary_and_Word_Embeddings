INFO: Pandarallel will run on 48 workers.
INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.
2022-07-31 15:32:35.270794: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Params: {'model_checkpoint': 'xlm-roberta-large', 'source_column': 'gloss', 'max_len': 150, 'batch_size': 16, 'dropout': 0.1, 'learning_rate': 0.0001, 'num_epochs': 150, 'early_stopping_limit': 5, 'device': 'cuda:0', 'loss_fn_name': 'cosine', 'emb_type': 'muse', 'use_adapters': True, 'resume_from_checkpoint': False, 'output_size': 300, 'knn_measure': 'cosine', 'num_warmup_steps': 300, 'extra_data': False}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31444/31444 [00:00<00:00, 38220.08it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4593/4593 [00:00<00:00, 21523.05it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4529/4529 [00:00<00:00, 42404.35it/s]
Len of Train before drop duplicates - 23933
Len of Val before drop duplicates - 3561
Len of Test before drop duplicates - 3464
Len of Train - 23933
Len of Val - 3561
Len of Test - 3464
muse
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 23933 entries, 0 to 23932
Data columns (total 3 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   word    23933 non-null  object
 1   gloss   23933 non-null  object
 2   muse    23933 non-null  object
dtypes: object(3)
memory usage: 561.1+ KB
None
             word                                              gloss                                               muse
423          Okie                           A person from Oklahoma .  [0.0112928, -0.0658679, 0.0242376, 0.0121888, ...
22190       troth                           Truth ; something true .  [0.105168, 0.0280807, -0.0883594, 0.0798127, -...
5533         crib  A job , a position ; ( British ) , an appointm...  [-0.0329332, -0.0178425, -0.0277359, 0.12134, ...
14204      nearby                         adjacent , near , close by  [0.0294672, -0.00948268, 0.0285052, 0.044563, ...
8331      extinct         No longer used ; obsolete , discontinued .  [-0.0525059, -0.0467622, -0.0806839, 0.0259632...
16296  positional  Having or pertaining to a value that is a func...  [-0.0135821, 0.0196561, -0.0232656, 0.0516228,...

Loading Tokenizer: xlm-roberta-large ...
Save checkpoint path: ../checkpoints/xlm-roberta-large_10
Train len 1496, val len: 223

Using device cuda:0 for training...
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Creating optimizer with Learning Rate: 0.0001

Training for 150 epochs with early stopping set to 5


num_training steps : 224400
  0%|                                                                                                                                       | 0/224 [00:00<?, ?it/s][Epoch 0 / 150]
  0%|▌                                                                                                               | 1/224 [02:03<7:38:08, 123.27s/it, loss=0.606]
Epoch: 0 	Training Loss: 0.577191 	Validation Loss: 0.508516
Loss improved saving checkpoint...
[Epoch 1 / 150]
  1%|█                                                                                                               | 2/224 [04:16<7:57:22, 129.02s/it, loss=0.498]
Epoch: 1 	Training Loss: 0.490509 	Validation Loss: 0.484777
Loss improved saving checkpoint...
[Epoch 2 / 150]
  2%|██                                                                                                              | 4/224 [08:30<7:45:34, 126.98s/it, loss=0.471]
Epoch: 2 	Training Loss: 0.468826 	Validation Loss: 0.476493
Loss improved saving checkpoint...
[Epoch 3 / 150]
  2%|██▌                                                                                                             | 5/224 [10:39<7:47:02, 127.96s/it, loss=0.456]
Epoch: 3 	Training Loss: 0.455540 	Validation Loss: 0.471328
Loss improved saving checkpoint...
[Epoch 4 / 150]
  3%|███▌                                                                                                            | 7/224 [14:53<7:37:40, 126.55s/it, loss=0.445]
Epoch: 4 	Training Loss: 0.445311 	Validation Loss: 0.469513
Loss improved saving checkpoint...
[Epoch 5 / 150]
  4%|████                                                                                                            | 8/224 [17:06<7:43:15, 128.68s/it, loss=0.436]
Epoch: 5 	Training Loss: 0.436391 	Validation Loss: 0.467873
Loss improved saving checkpoint...
[Epoch 6 / 150]
  4%|████▉                                                                                                          | 10/224 [21:16<7:27:47, 125.55s/it, loss=0.428]
Epoch: 6 	Training Loss: 0.428254 	Validation Loss: 0.468195
[Epoch 7 / 150]
  5%|█████▍                                                                                                         | 11/224 [23:21<7:24:43, 125.28s/it, loss=0.418]
Epoch: 7 	Training Loss: 0.420837 	Validation Loss: 0.465736
Loss improved saving checkpoint...
[Epoch 8 / 150]
  6%|██████▍                                                                                                        | 13/224 [27:37<7:23:25, 126.09s/it, loss=0.414]
Epoch: 8 	Training Loss: 0.413751 	Validation Loss: 0.466197
[Epoch 9 / 150]
  6%|██████▉                                                                                                        | 14/224 [29:44<7:22:10, 126.34s/it, loss=0.404]
Epoch: 9 	Training Loss: 0.407422 	Validation Loss: 0.467177
[Epoch 10 / 150]
  7%|████████                                                                                                         | 16/224 [33:52<7:12:41, 124.82s/it, loss=0.4]
Epoch: 10 	Training Loss: 0.401622 	Validation Loss: 0.471318
[Epoch 11 / 150]
  8%|████████▍                                                                                                      | 17/224 [36:03<7:17:08, 126.71s/it, loss=0.394]
Epoch: 11 	Training Loss: 0.395775 	Validation Loss: 0.469934
[Epoch 12 / 150]
  8%|█████████▌                                                                                                      | 19/224 [40:13<7:07:54, 125.24s/it, loss=0.39]
Epoch: 12 	Training Loss: 0.390598 	Validation Loss: 0.471492

Loss did not reduce for last 5 epochs. Stopped training..
  8%|█████████▌