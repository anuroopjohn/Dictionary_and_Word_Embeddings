2022-07-29 09:32:40.553839: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
INFO: Pandarallel will run on 48 workers.
INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.
  0%|          | 0/31444 [00:00<?, ?it/s] 15%|█▍        | 4563/31444 [00:00<00:00, 45624.39it/s] 29%|██▉       | 9202/31444 [00:00<00:00, 46073.01it/s] 45%|████▍     | 14032/31444 [00:00<00:00, 47087.20it/s] 60%|██████    | 19019/31444 [00:00<00:00, 48182.23it/s] 76%|███████▋  | 23990/31444 [00:00<00:00, 48729.86it/s] 92%|█████████▏| 28984/31444 [00:00<00:00, 49140.13it/s]100%|██████████| 31444/31444 [00:00<00:00, 48430.30it/s]
  0%|          | 0/4593 [00:00<?, ?it/s]100%|██████████| 4593/4593 [00:00<00:00, 50607.33it/s]
  0%|          | 0/4529 [00:00<?, ?it/s]100%|██████████| 4529/4529 [00:00<00:00, 49167.35it/s]
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Len of Train before drop duplicates - 28983
Len of Val before drop duplicates - 4219
Len of Test before drop duplicates - 4227
Len of Train - 28983
Len of Val - 4219
Len of Test - 4227
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 28983 entries, 0 to 28982
Data columns (total 3 columns):
 #   Column    Non-Null Count  Dtype 
---  ------    --------------  ----- 
 0   word      28983 non-null  object
 1   gloss     28983 non-null  object
 2   fasttext  28983 non-null  object
dtypes: object(3)
memory usage: 679.4+ KB
None
              word  ...                                           fasttext
27154  underground  ...  [-0.2013, -0.3014, 0.1525, 0.1562, -0.304, -0....
20981    refection  ...  [0.0731, 0.2905, -0.5642, -0.0788, -0.0255, 0....
22957      shelter  ...  [-0.0753, -0.5014, -0.2117, -0.1908, -0.3724, ...
13664   indecisive  ...  [-0.0403, 0.0665, 0.1213, -0.7659, 0.5997, 0.0...
2929       bedevil  ...  [0.1295, -0.0428, 0.0991, -0.1387, 0.3032, 0.1...
24343        stage  ...  [0.0975, -0.3233, -0.1921, -0.0952, 0.0622, 0....

[6 rows x 3 columns]
{'model_checkpoint': 'xlm-roberta-large', 'source_column': 'gloss', 'max_len': 150, 'batch_size': 16, 'dropout': 0.1, 'learning_rate': 0.0001, 'num_epochs': 150, 'early_stopping_limit': 5, 'device': 'cuda:0', 'loss_fn_name': 'cosine', 'emb_type': 'fasttext', 'use_adapters': False, 'resume_from_checkpoint': False, 'output_size': 300, 'knn_measure': 'cosine', 'num_warmup_steps': 300, 'extra_data': False}

Loading Tokenizer: xlm-roberta-large ...
Save checkpoint path: ../checkpoints/xlm-roberta-large_cosineloss_fasttext_embs_False_adapter_False_extradata_0
Train len 1812, val len: 264

Using device cuda:0 for training...

Creating optimizer with Learning Rate: 0.0001

Training for 150 epochs with early stopping set to 5


num_training steps : 271800
  0%|          | 0/271 [00:00<?, ?it/s]  0%|          | 1/271 [03:22<15:13:01, 202.89s/it]  0%|          | 1/271 [03:22<15:13:01, 202.89s/it, loss=0.651]  1%|          | 2/271 [06:47<15:14:15, 203.92s/it, loss=0.651]  1%|          | 2/271 [06:47<15:14:15, 203.92s/it, loss=0.647]  1%|          | 3/271 [09:45<14:18:50, 192.28s/it, loss=0.647]  1%|          | 3/271 [09:45<14:18:50, 192.28s/it, loss=0.648]  1%|▏         | 4/271 [13:03<14:24:00, 194.16s/it, loss=0.648]  1%|▏         | 4/271 [13:03<14:24:00, 194.16s/it, loss=0.647]  2%|▏         | 5/271 [15:46<13:31:57, 183.15s/it, loss=0.647]  2%|▏         | 5/271 [15:46<13:31:57, 183.15s/it, loss=0.647]  2%|▏         | 6/271 [19:07<13:55:51, 189.25s/it, loss=0.647]  2%|▏         | 6/271 [19:07<13:55:51, 189.25s/it, loss=0.647]  3%|▎         | 7/271 [22:00<13:28:18, 183.71s/it, loss=0.647]  3%|▎         | 7/271 [22:00<13:28:18, 183.71s/it, loss=0.647]  3%|▎         | 8/271 [25:15<13:41:08, 187.33s/it, loss=0.647]  3%|▎         | 8/271 [25:15<13:41:08, 187.33s/it, loss=0.646]  3%|▎         | 9/271 [27:58<13:05:25, 179.87s/it, loss=0.646]  3%|▎         | 9/271 [27:58<13:05:25, 179.87s/it, loss=0.647]  4%|▎         | 10/271 [31:08<13:15:53, 182.96s/it, loss=0.647]  4%|▎         | 10/271 [31:08<13:15:53, 182.96s/it, loss=0.647]  4%|▍         | 11/271 [34:16<13:19:57, 184.61s/it, loss=0.647]  4%|▍         | 11/271 [34:16<13:19:57, 184.61s/it, loss=0.645]  4%|▍         | 12/271 [37:19<13:14:14, 184.00s/it, loss=0.645]  4%|▍         | 12/271 [37:19<13:14:14, 184.00s/it, loss=0.646]  5%|▍         | 13/271 [40:28<13:18:11, 185.63s/it, loss=0.646]  5%|▍         | 13/271 [40:28<13:18:11, 185.63s/it, loss=0.647]  5%|▌         | 14/271 [43:39<13:21:06, 187.03s/it, loss=0.647]  5%|▌         | 14/271 [43:39<13:21:06, 187.03s/it, loss=0.646]  6%|▌         | 15/271 [46:33<13:02:09, 183.32s/it, loss=0.646]  6%|▌         | 15/271 [46:33<13:02:09, 183.32s/it, loss=0.646]  6%|▌         | 16/271 [49:36<12:58:41, 183.22s/it, loss=0.646]  6%|▌         | 16/271 [49:36<12:58:41, 183.22s/it, loss=0.646][Epoch 0 / 150]

Epoch: 0 	Training Loss: 0.649830 	Validation Loss: 0.647462
Loss improved saving checkpoint... 
[Epoch 1 / 150]

Epoch: 1 	Training Loss: 0.647475 	Validation Loss: 0.647304
Loss improved saving checkpoint... 
[Epoch 2 / 150]

Epoch: 2 	Training Loss: 0.647046 	Validation Loss: 0.647005
Loss improved saving checkpoint... 
[Epoch 3 / 150]

Epoch: 3 	Training Loss: 0.646886 	Validation Loss: 0.646942
Loss improved saving checkpoint... 
[Epoch 4 / 150]

Epoch: 4 	Training Loss: 0.646707 	Validation Loss: 0.646945
[Epoch 5 / 150]

Epoch: 5 	Training Loss: 0.646571 	Validation Loss: 0.646952
[Epoch 6 / 150]

Epoch: 6 	Training Loss: 0.646390 	Validation Loss: 0.647155
[Epoch 7 / 150]

Epoch: 7 	Training Loss: 0.646385 	Validation Loss: 0.647092
[Epoch 8 / 150]

Epoch: 8 	Training Loss: 0.646262 	Validation Loss: 0.647248

Loss did not reduce for last 5 epochs. Stopped training..
  6%|▌         | 16/271 [50:40<13:27:33, 190.01s/it, loss=0.646]