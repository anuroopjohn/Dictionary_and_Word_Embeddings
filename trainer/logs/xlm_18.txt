2022-08-06 00:06:04.475987: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Params: {'model_checkpoint': 'xlm-roberta-large', 'source_column': 'gloss', 'max_len': 150, 'batch_size': 16, 'dropout': 0.1, 'learning_rate': 0.0001, 'num_epochs': 150, 'early_stopping_limit': 5, 'device': 'cuda:0', 'loss_fn_name': 'cosine', 'emb_type': 'xlm-roberta-large', 'use_adapters': True, 'resume_from_checkpoint': False, 'output_size': 1024, 'knn_measure': 'cosine', 'num_warmup_steps': 300, 'extra_data': False}
1024 <class 'int'>
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31444/31444 [00:02<00:00, 14594.69it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4593/4593 [00:00<00:00, 16760.12it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4529/4529 [00:00<00:00, 16565.81it/s]
Len of Train before drop duplicates - 30354
Len of Val before drop duplicates - 4535
Len of Test before drop duplicates - 4442
Len of Train - 30354
Len of Val - 4535
Len of Test - 4442
xlm-roberta-large
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 30354 entries, 0 to 30353
Data columns (total 3 columns):
 #   Column             Non-Null Count  Dtype
---  ------             --------------  -----
 0   word               30354 non-null  object
 1   gloss              30354 non-null  object
 2   xlm-roberta-large  30354 non-null  object
dtypes: object(3)
memory usage: 711.5+ KB
None
                word                                              gloss                                  xlm-roberta-large
18729     one-to-one  Matching each member of one set with exactly o...  [0.03443345036357641, -0.11034621000289917, -0...
22269      repenting                                         repentance  [0.0415712408721447, -0.0680927224457264, -0.0...
28222  unaccountable  not responsible ; free from accountability or ...  [-0.09593986719846725, -0.14244864881038666, 0...
210          Clemson  To uncharacteristically poorly , well below ex...  [-0.1035331351061662, 0.016775610856711864, -0...
19408    paperweight  A small , decorative , somewhat weighty object...  [0.06872848863713443, 0.16939719021320343, -0....
16302       lordship  Title applied to a lord , bishop , judge , or ...  [0.03591446826855342, -0.055265065719140694, 0...

Loading Tokenizer: xlm-roberta-large ...
Save checkpoint path: ../checkpoints/xlm-roberta-large_xlm-roberta-large_0
Train len 1898, val len: 284

Using device cuda:0 for training...
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model parameters count: 3428480

Creating optimizer with Learning Rate: 0.0001

Training for 150 epochs with early stopping set to 5


num_training steps : 284700
  0%|                                                                                                                                                                                                    | 0/284 [00:00<?, ?it/s][Epoch 0 / 150]
  0%|▌                                                                                                                                                                            | 1/284 [02:41<12:39:33, 161.04s/it, loss=0.12]
Epoch: 0 	Training Loss: 0.070445 	Validation Loss: 0.009133
Loss improved saving checkpoint...
[Epoch 1 / 150]
  1%|█▊                                                                                                                                                                         | 3/284 [07:49<12:02:04, 154.18s/it, loss=0.0115]
Epoch: 1 	Training Loss: 0.010802 	Validation Loss: 0.009009
Loss improved saving checkpoint...
[Epoch 2 / 150]
  2%|██▉                                                                                                                                                                       | 5/284 [13:05<12:06:15, 156.19s/it, loss=0.00919]
Epoch: 2 	Training Loss: 0.009116 	Validation Loss: 0.009007
Loss improved saving checkpoint...
[Epoch 3 / 150]
  2%|████▏                                                                                                                                                                      | 7/284 [18:23<12:05:57, 157.25s/it, loss=0.0089]
Epoch: 3 	Training Loss: 0.008860 	Validation Loss: 0.008755
Loss improved saving checkpoint...
[Epoch 4 / 150]
  3%|█████▍                                                                                                                                                                    | 9/284 [23:29<11:47:23, 154.34s/it, loss=0.00858]
Epoch: 4 	Training Loss: 0.008563 	Validation Loss: 0.008460
Loss improved saving checkpoint...
[Epoch 5 / 150]
  4%|██████▌                                                                                                                                                                  | 11/284 [28:43<11:45:30, 155.06s/it, loss=0.00837]
Epoch: 5 	Training Loss: 0.008359 	Validation Loss: 0.008314
Loss improved saving checkpoint...
[Epoch 6 / 150]
  5%|███████▋                                                                                                                                                                 | 13/284 [34:05<11:50:17, 157.26s/it, loss=0.00819]
Epoch: 6 	Training Loss: 0.008197 	Validation Loss: 0.008243
Loss improved saving checkpoint...
[Epoch 7 / 150]
  5%|████████▉                                                                                                                                                                | 15/284 [39:37<11:59:49, 160.56s/it, loss=0.00805]
Epoch: 7 	Training Loss: 0.008046 	Validation Loss: 0.008148
Loss improved saving checkpoint...
[Epoch 8 / 150]
  6%|██████████                                                                                                                                                               | 17/284 [45:08<12:01:26, 162.12s/it, loss=0.00791]
Epoch: 8 	Training Loss: 0.007910 	Validation Loss: 0.008126
Loss improved saving checkpoint...
[Epoch 9 / 150]
  6%|██████████▋                                                                                                                                                              | 18/284 [47:57<12:08:13, 164.26s/it, loss=0.00778]
Epoch: 9 	Training Loss: 0.007776 	Validation Loss: 0.008120
Loss improved saving checkpoint...
[Epoch 10 / 150]
  7%|███████████▉                                                                                                                                                             | 20/284 [52:54<11:22:24, 155.09s/it, loss=0.00761]
Epoch: 10 	Training Loss: 0.007642 	Validation Loss: 0.008133
[Epoch 11 / 150]
  8%|█████████████                                                                                                                                                            | 22/284 [57:43<10:50:01, 148.86s/it, loss=0.00749]
Epoch: 11 	Training Loss: 0.007497 	Validation Loss: 0.008593
[Epoch 12 / 150]
  8%|██████████████                                                                                                                                                         | 24/284 [1:02:28<10:29:09, 145.19s/it, loss=0.00732]
Epoch: 12 	Training Loss: 0.007360 	Validation Loss: 0.008412
[Epoch 13 / 150]
  9%|███████████████▎                                                                                                                                                       | 26/284 [1:07:14<10:17:20, 143.57s/it, loss=0.00722]
Epoch: 13 	Training Loss: 0.007238 	Validation Loss: 0.008350
[Epoch 14 / 150]
 10%|████████████████▍                                                                                                                                                      | 28/284 [1:12:00<10:08:54, 142.71s/it, loss=0.00712]
Epoch: 14 	Training Loss: 0.007117 	Validation Loss: 0.008444

Loss did not reduce for last 5 epochs. Stopped training..
 10%|████████████████▍                                                                                                                                                      | 28/284 [1:13:18<11:10:16, 157.09s/it, loss=0.00712]
