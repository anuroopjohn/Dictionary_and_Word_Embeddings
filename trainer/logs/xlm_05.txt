INFO: Pandarallel will run on 48 workers.
INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.
2022-07-29 13:38:26.725062: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31444/31444 [00:00<00:00, 38470.27it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4593/4593 [00:00<00:00, 32594.03it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4529/4529 [00:00<00:00, 35656.10it/s]
Len of Train before drop duplicates - 28983
Len of Val before drop duplicates - 4219
Len of Test before drop duplicates - 4227
Len of Train - 28983
Len of Val - 4219
Len of Test - 4227
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 28983 entries, 0 to 28982
Data columns (total 3 columns):
 #   Column    Non-Null Count  Dtype
---  ------    --------------  -----
 0   word      28983 non-null  object
 1   gloss     28983 non-null  object
 2   fasttext  28983 non-null  object
dtypes: object(3)
memory usage: 679.4+ KB
None
              word                                              gloss                                           fasttext
27154  underground  A movement or organisation of people who resis...  [-0.2013, -0.3014, 0.1525, 0.1562, -0.304, -0....
20981    refection  Physical refreshment , especially with food or...  [0.0731, 0.2905, -0.5642, -0.0788, -0.0255, 0....
22957      shelter                                    To take cover .  [-0.0753, -0.5014, -0.2117, -0.1908, -0.3724, ...
13664   indecisive                          inconclusive or uncertain  [-0.0403, 0.0665, 0.1213, -0.7659, 0.5997, 0.0...
2929       bedevil       To harass or cause trouble for ; to plague .  [0.1295, -0.0428, 0.0991, -0.1387, 0.3032, 0.1...
24343        stage          To place in position to prepare for use .  [0.0975, -0.3233, -0.1921, -0.0952, 0.0622, 0....
Params: {'model_checkpoint': 'xlm-roberta-large', 'source_column': 'gloss', 'max_len': 150, 'batch_size': 16, 'dropout': 0.1, 'learning_rate': 0.0001, 'num_epochs': 150, 'early_stopping_limit': 5, 'device': 'cuda:0', 'loss_fn_name': 'cosine', 'emb_type': 'fasttext', 'use_adapters': True, 'resume_from_checkpoint': False, 'output_size': 300, 'knn_measure': 'cosine', 'num_warmup_steps': 300, 'extra_data': False}

Loading Tokenizer: xlm-roberta-large ...
Save checkpoint path: ../checkpoints/xlm-roberta-large_4
Train len 1812, val len: 264

Using device cuda:0 for training...
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

Creating optimizer with Learning Rate: 0.0001

Training for 150 epochs with early stopping set to 5


num_training steps : 271800
  0%|                                                                                                                                       | 0/271 [00:00<?, ?it/s][Epoch 0 / 150]
  0%|▍                                                                                                                | 1/271 [00:58<4:21:16, 58.06s/it, loss=0.681]
Epoch: 0 	Training Loss: 0.643962 	Validation Loss: 0.584820
Loss improved saving checkpoint...
[Epoch 1 / 150]
  1%|█▎                                                                                                               | 3/271 [03:05<4:35:25, 61.66s/it, loss=0.576]
Epoch: 1 	Training Loss: 0.572354 	Validation Loss: 0.562398
Loss improved saving checkpoint...
[Epoch 2 / 150]
  2%|██                                                                                                               | 5/271 [05:16<4:39:08, 62.96s/it, loss=0.554]
Epoch: 2 	Training Loss: 0.553331 	Validation Loss: 0.550986
Loss improved saving checkpoint...
[Epoch 3 / 150]
  3%|██▉                                                                                                              | 7/271 [07:23<4:35:13, 62.55s/it, loss=0.542]
Epoch: 3 	Training Loss: 0.541469 	Validation Loss: 0.544534
Loss improved saving checkpoint...
[Epoch 4 / 150]
  3%|███▊                                                                                                             | 9/271 [09:32<4:34:32, 62.87s/it, loss=0.532]
Epoch: 4 	Training Loss: 0.532455 	Validation Loss: 0.541137
Loss improved saving checkpoint...
[Epoch 5 / 150]
  4%|████▏                                                                                                           | 10/271 [10:46<4:47:15, 66.04s/it, loss=0.526]
Epoch: 5 	Training Loss: 0.525304 	Validation Loss: 0.538174
Loss improved saving checkpoint...
[Epoch 6 / 150]
  4%|█████                                                                                                            | 12/271 [12:54<4:36:48, 64.12s/it, loss=0.52]
Epoch: 6 	Training Loss: 0.519107 	Validation Loss: 0.536531
Loss improved saving checkpoint...
[Epoch 7 / 150]
  5%|█████▊                                                                                                          | 14/271 [15:02<4:31:26, 63.37s/it, loss=0.514]
Epoch: 7 	Training Loss: 0.513643 	Validation Loss: 0.533664
Loss improved saving checkpoint...
[Epoch 8 / 150]
  6%|██████▌                                                                                                         | 16/271 [17:07<4:25:22, 62.44s/it, loss=0.509]
Epoch: 8 	Training Loss: 0.509180 	Validation Loss: 0.533089
Loss improved saving checkpoint...
[Epoch 9 / 150]
  7%|███████▍                                                                                                        | 18/271 [19:17<4:24:45, 62.79s/it, loss=0.505]
Epoch: 9 	Training Loss: 0.504961 	Validation Loss: 0.531684
Loss improved saving checkpoint...
[Epoch 10 / 150]
  7%|███████▉                                                                                                          | 19/271 [20:28<4:35:18, 65.55s/it, loss=0.5]
Epoch: 10 	Training Loss: 0.501000 	Validation Loss: 0.531264
Loss improved saving checkpoint...
[Epoch 11 / 150]
  8%|████████▋                                                                                                       | 21/271 [22:33<4:23:50, 63.32s/it, loss=0.497]
Epoch: 11 	Training Loss: 0.497546 	Validation Loss: 0.529352
Loss improved saving checkpoint...
[Epoch 12 / 150]
  8%|█████████▌                                                                                                      | 23/271 [24:41<4:20:19, 62.98s/it, loss=0.493]
Epoch: 12 	Training Loss: 0.494271 	Validation Loss: 0.529185
Loss improved saving checkpoint...
[Epoch 13 / 150]
  9%|██████████▎                                                                                                     | 25/271 [26:51<4:19:40, 63.34s/it, loss=0.491]
Epoch: 13 	Training Loss: 0.491320 	Validation Loss: 0.529099
Loss improved saving checkpoint...
[Epoch 14 / 150]
 10%|███████████▏                                                                                                    | 27/271 [28:59<4:16:19, 63.03s/it, loss=0.488]
Epoch: 14 	Training Loss: 0.488570 	Validation Loss: 0.528369
Loss improved saving checkpoint...
[Epoch 15 / 150]
 10%|███████████▌                                                                                                    | 28/271 [30:12<4:27:33, 66.06s/it, loss=0.484]
Epoch: 15 	Training Loss: 0.485797 	Validation Loss: 0.528687
[Epoch 16 / 150]
 11%|████████████▍                                                                                                   | 30/271 [32:14<4:12:44, 62.92s/it, loss=0.482]
Epoch: 16 	Training Loss: 0.483294 	Validation Loss: 0.528581
[Epoch 17 / 150]
 12%|█████████████▎                                                                                                   | 32/271 [34:20<4:09:35, 62.66s/it, loss=0.48]
Epoch: 17 	Training Loss: 0.480955 	Validation Loss: 0.529191
[Epoch 18 / 150]
 13%|██████████████                                                                                                  | 34/271 [36:21<4:01:32, 61.15s/it, loss=0.478]
Epoch: 18 	Training Loss: 0.478493 	Validation Loss: 0.528376
[Epoch 19 / 150]
 13%|██████████████▉                                                                                                 | 36/271 [38:24<3:58:52, 60.99s/it, loss=0.477]
Epoch: 19 	Training Loss: 0.476944 	Validation Loss: 0.528589

Loss did not reduce for last 5 epochs. Stopped training..