Params: {'model_checkpoint': 'xlm-roberta-large', 'source_column': 'gloss', 'max_len': 150, 'batch_size': 16, 'dropout': 0.1, 'learning_rate': 0.0001, 'num_epochs': 150, 'early_stopping_limit': 5, 'device': 'cuda:0', 'loss_fn_name': 'cosine', 'emb_type': 'fasttext', 'use_adapters': False, 'resume_from_checkpoint': False, 'output_size': 300, 'knn_measure': 'cosine', 'num_warmup_steps': 300, 'extra_data': False}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31444/31444 [00:00<00:00, 45591.03it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4593/4593 [00:00<00:00, 42049.61it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4529/4529 [00:00<00:00, 44350.54it/s]
Len of Train before drop duplicates - 28983
Len of Val before drop duplicates - 4219
Len of Test before drop duplicates - 4227
Len of Train - 28983
Len of Val - 4219
Len of Test - 4227
fasttext
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 28983 entries, 0 to 28982
Data columns (total 3 columns):
 #   Column    Non-Null Count  Dtype
---  ------    --------------  -----
 0   word      28983 non-null  object
 1   gloss     28983 non-null  object
 2   fasttext  28983 non-null  object
dtypes: object(3)
memory usage: 679.4+ KB
None
              word                                              gloss                                           fasttext
27154  underground  A movement or organisation of people who resis...  [-0.2013, -0.3014, 0.1525, 0.1562, -0.304, -0....
20981    refection  Physical refreshment , especially with food or...  [0.0731, 0.2905, -0.5642, -0.0788, -0.0255, 0....
22957      shelter                                    To take cover .  [-0.0753, -0.5014, -0.2117, -0.1908, -0.3724, ...
13664   indecisive                          inconclusive or uncertain  [-0.0403, 0.0665, 0.1213, -0.7659, 0.5997, 0.0...
2929       bedevil       To harass or cause trouble for ; to plague .  [0.1295, -0.0428, 0.0991, -0.1387, 0.3032, 0.1...
24343        stage          To place in position to prepare for use .  [0.0975, -0.3233, -0.1921, -0.0952, 0.0622, 0....

Loading Tokenizer: xlm-roberta-large ...
Save checkpoint path: ../checkpoints/xlm-roberta-large_15
Train len 1812, val len: 264

Using device cuda:0 for training...
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model parameters count: 102126892

Creating optimizer with Learning Rate: 0.0001

Training for 150 epochs with early stopping set to 5

for name, param in model.named_parameters():
   if name.startswith("base.encoder.layer") and int(name.split('.')[3])<16:
        param.requires_grad = False
   elif name.startswith('base.embeddings'):
        param.requires_grad = False
   elif name.startswith('base.pooler'):
        param.requires_grad = False

num_training steps : 271800
  0%|                                                                                                                                                                                      | 0/271 [00:00<?, ?it/s][Epoch 0 / 150]
  0%|▌                                                                                                                                                              | 1/271 [01:45<7:56:44, 105.94s/it, loss=0.601]
Epoch: 0 	Training Loss: 0.569663 	Validation Loss: 0.523574
Loss improved saving checkpoint...
[Epoch 1 / 150]
  1%|█▊                                                                                                                                                               | 3/271 [05:38<8:24:46, 113.01s/it, loss=0.5]
Epoch: 1 	Training Loss: 0.499563 	Validation Loss: 0.506235
Loss improved saving checkpoint...
[Epoch 2 / 150]
  2%|██▉                                                                                                                                                            | 5/271 [09:34<8:29:18, 114.88s/it, loss=0.469]
Epoch: 2 	Training Loss: 0.468889 	Validation Loss: 0.500710
Loss improved saving checkpoint...
[Epoch 3 / 150]
  3%|████                                                                                                                                                           | 7/271 [13:26<8:24:01, 114.55s/it, loss=0.442]
Epoch: 3 	Training Loss: 0.443448 	Validation Loss: 0.498161
Loss improved saving checkpoint...
[Epoch 4 / 150]
  3%|█████▎                                                                                                                                                         | 9/271 [17:15<8:16:58, 113.81s/it, loss=0.417]
Epoch: 4 	Training Loss: 0.417766 	Validation Loss: 0.498644
[Epoch 5 / 150]
  4%|█████▊                                                                                                                                                        | 10/271 [19:14<8:21:40, 115.33s/it, loss=0.388]
Epoch: 5 	Training Loss: 0.392065 	Validation Loss: 0.504479
[Epoch 6 / 150]
  4%|██████▉                                                                                                                                                       | 12/271 [22:44<7:50:21, 108.96s/it, loss=0.363]
Epoch: 6 	Training Loss: 0.367444 	Validation Loss: 0.508158
[Epoch 7 / 150]
  5%|████████▎                                                                                                                                                       | 14/271 [25:32<6:47:26, 95.12s/it, loss=0.34]
Epoch: 7 	Training Loss: 0.344022 	Validation Loss: 0.512408
[Epoch 8 / 150]
  6%|█████████▍                                                                                                                                                     | 16/271 [28:06<6:02:49, 85.37s/it, loss=0.321]
Epoch: 8 	Training Loss: 0.322272 	Validation Loss: 0.516837

Loss did not reduce for last 5 epochs. Stopped training..
  6%|█████████▎                                                                                                                                                    | 16/271 [28:37<7:36:08, 107.33s/it, loss=0.321]