{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d910fa5-92b3-4fe2-8e4e-db8fce931d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81f5507-a13c-4a4e-a35c-49641d8e5ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 48 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 21:47:28.511683: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "#os.environ['HF_HOME'] = '/data/users/ugarg/hf/hf_cache/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data/users/ugarg/hf/hf_cache/'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "#sys.path.append('../../..')\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=False)\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import AdamW, AutoTokenizer,  AutoModel\n",
    "from torch.nn.functional import one_hot\n",
    "from collections import Counter\n",
    "from torch.optim import AdamW, Adam\n",
    "from transformers import get_scheduler, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(42)\n",
    "import joblib\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from utils.get_data_and_splits import LoadData\n",
    "from utils.params import get_roberta_params, cget_xlm_params\n",
    "from utils.model import Model\n",
    "from utils.utils import checkpoint_builder\n",
    "from utils.faiss_utils import create_and_store_index, get_top_n_accuracy\n",
    "from utils.predict import prep_model, predict_on_batch #\n",
    "\n",
    "from CreatePytorchDataset import TrainDataset, ValDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d14dff2-f533-4b80-9efc-da08c9e2048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31444/31444 [00:00<00:00, 38088.43it/s]\n",
      "100%|██████████| 4593/4593 [00:00<00:00, 42973.83it/s]\n",
      "100%|██████████| 4529/4529 [00:00<00:00, 28849.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of Train before drop duplicates - 28983\n",
      "Len of Val before drop duplicates - 4219\n",
      "Len of Test before drop duplicates - 4227\n",
      "Len of Train - 28983\n",
      "Len of Val - 4219\n",
      "Len of Test - 4227\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28983 entries, 0 to 28982\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   word      28983 non-null  object\n",
      " 1   gloss     28983 non-null  object\n",
      " 2   fasttext  28983 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 679.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>gloss</th>\n",
       "      <th>fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27154</th>\n",
       "      <td>underground</td>\n",
       "      <td>A movement or organisation of people who resis...</td>\n",
       "      <td>[-0.2013, -0.3014, 0.1525, 0.1562, -0.304, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20981</th>\n",
       "      <td>refection</td>\n",
       "      <td>Physical refreshment , especially with food or...</td>\n",
       "      <td>[0.0731, 0.2905, -0.5642, -0.0788, -0.0255, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22957</th>\n",
       "      <td>shelter</td>\n",
       "      <td>To take cover .</td>\n",
       "      <td>[-0.0753, -0.5014, -0.2117, -0.1908, -0.3724, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13664</th>\n",
       "      <td>indecisive</td>\n",
       "      <td>inconclusive or uncertain</td>\n",
       "      <td>[-0.0403, 0.0665, 0.1213, -0.7659, 0.5997, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>bedevil</td>\n",
       "      <td>To harass or cause trouble for ; to plague .</td>\n",
       "      <td>[0.1295, -0.0428, 0.0991, -0.1387, 0.3032, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24343</th>\n",
       "      <td>stage</td>\n",
       "      <td>To place in position to prepare for use .</td>\n",
       "      <td>[0.0975, -0.3233, -0.1921, -0.0952, 0.0622, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word                                              gloss  \\\n",
       "27154  underground  A movement or organisation of people who resis...   \n",
       "20981    refection  Physical refreshment , especially with food or...   \n",
       "22957      shelter                                    To take cover .   \n",
       "13664   indecisive                          inconclusive or uncertain   \n",
       "2929       bedevil       To harass or cause trouble for ; to plague .   \n",
       "24343        stage          To place in position to prepare for use .   \n",
       "\n",
       "                                                fasttext  \n",
       "27154  [-0.2013, -0.3014, 0.1525, 0.1562, -0.304, -0....  \n",
       "20981  [0.0731, 0.2905, -0.5642, -0.0788, -0.0255, 0....  \n",
       "22957  [-0.0753, -0.5014, -0.2117, -0.1908, -0.3724, ...  \n",
       "13664  [-0.0403, 0.0665, 0.1213, -0.7659, 0.5997, 0.0...  \n",
       "2929   [0.1295, -0.0428, 0.0991, -0.1387, 0.3032, 0.1...  \n",
       "24343  [0.0975, -0.3233, -0.1921, -0.0952, 0.0622, 0....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaddata = LoadData()\n",
    "train, val, test = loaddata.get_data(\n",
    "        extra_data=False,\n",
    "        extra_data_path=\"/data/users/abose1/Capstone/Dictionary_and_Word_Embeddings/data/static_wiki.pkl\"       \n",
    "        ) #removes non-zero,\n",
    "\n",
    "print (train[['word','gloss', 'fasttext']].info())\n",
    "\n",
    "train.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7321c1c6-a93f-4e71-9d39-c3dd57e09ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = joblib.load('../data/clean_data/train.joblib')\n",
    "# val = joblib.load('../data/clean_data/val.joblib')\n",
    "# test = joblib.load('../data/clean_data/test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabcbaa0-7c5a-43bc-97ae-261b6a5502d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_checkpoint': 'xlm-roberta-large',\n",
       " 'source_column': 'gloss',\n",
       " 'max_len': 150,\n",
       " 'batch_size': 64,\n",
       " 'dropout': 0.1,\n",
       " 'learning_rate': 0.001,\n",
       " 'num_epochs': 150,\n",
       " 'early_stopping_limit': 5,\n",
       " 'device': 'cuda:0',\n",
       " 'loss_fn_name': 'cosine',\n",
       " 'emb_type': 'fasttext',\n",
       " 'use_adapters': True,\n",
       " 'resume_from_checkpoint': False,\n",
       " 'output_size': 300,\n",
       " 'knn_measure': 'cosine',\n",
       " 'extra_data': False,\n",
       " 'num_warmup_steps': 300}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_xlm_params()\n",
    "params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca89254d-a13d-4b2f-923e-78f6a86e36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.faiss_utils import create_and_store_index, get_top_n_accuracy\n",
    "\n",
    "# create_and_store_index(\n",
    "#     params['emb_type'], params['knn_measure'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dae01d2b-91bf-48c0-8a07-6293bd49c287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Tokenizer: xlm-roberta-large ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##################\n",
    "#   tokenizer\n",
    "##################\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = params['model_checkpoint']\n",
    "print(f'\\nLoading Tokenizer: {model_checkpoint} ...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a88d735-da20-4184-9c54-fcd43cc1f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint_path = f\"../checkpoints/{params['model_checkpoint']}_{params['loss_fn_name']}loss_{params['emb_type']}_embs_{params['use_adapters']}_adapter_{params['extra_data']}_extradata\"\n",
    "save_checkpoint_path\n",
    "\n",
    "i=1\n",
    "while True:\n",
    "    if os.path.exists(save_checkpoint_path):\n",
    "        print (f'Path {save_checkpoint_path} already exists.')\n",
    "        \n",
    "        save_checkpoint_path = save_checkpoint_path + '_' + str(i)\n",
    "        print(f'New path: {save_checkpoint_path}')\n",
    "        i+=1\n",
    "        \n",
    "    else:\n",
    "        os.mkdir(save_checkpoint_path)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6606ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(params,open(save_checkpoint_path+'/params.json','w'),indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4fc8c2-1963-452c-8ae2-25e8c6247c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(\n",
    "    train, \n",
    "    tokenizer, \n",
    "    params['source_column'],\n",
    "    params['max_len'], \n",
    "    params['emb_type']\n",
    ")\n",
    "val_dataset = ValDataset(\n",
    "    val, \n",
    "    tokenizer, \n",
    "    params['source_column'],\n",
    "    params['max_len'], \n",
    "    params['emb_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f09a393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23, 74, 33])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))['attention_mask'].sum(axis=1)#.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6297ed4e-44a5-4fe9-9b64-5672e0299f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len 9661, val len: 66\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn = data_collator,\n",
    "    batch_size = params['batch_size'],\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    collate_fn = data_collator, \n",
    "    batch_size=params['batch_size']\n",
    ")\n",
    "print (f'Train len {len(train_dataloader)}, val len: {len(val_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d7aedfa-e6a6-4849-abee-8ef64826dd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device cuda:0 for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "####################################\n",
    "#   Load Model\n",
    "####################################\n",
    "\n",
    "print(f\"\\nUsing device {params['device']} for training...\")\n",
    "\n",
    "model = Model(params['model_checkpoint'], \n",
    "              params['output_size'], \n",
    "              params['dropout'], \n",
    "              params['device'], \n",
    "              params['loss_fn_name'], \n",
    "              params['use_adapters'])\n",
    "model.to(params['device'])\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff0aeb90-e0bd-45d5-91b7-601c383cd421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating optimizer with Learning Rate: 0.001\n",
      "\n",
      "Training for 150 epochs with early stopping set to 5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "####################################\n",
    "#   Create Optimizer\n",
    "####################################\n",
    "print(f\"\\nCreating optimizer with Learning Rate: {params['learning_rate']}\")\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = params['learning_rate'])\n",
    "\n",
    "print(f\"\\nTraining for {params['num_epochs']} epochs with early stopping set to {params['early_stopping_limit']}\\n\\n\")\n",
    "\n",
    "num_train_epochs = params['num_epochs']\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=300,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b4bc3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['resume_from_checkpoint']=False\n",
    "if params['resume_from_checkpoint']:\n",
    "    print('Resuming from Checkpoint...')\n",
    "    print('Loading Checkpoint ...')\n",
    "    checkpoint = torch.load(save_checkpoint_path, map_location=params['device'])\n",
    "    print('Setting Models state to checkpoint...')\n",
    "    #assert checkpoint['model_params'] == model_params\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print('Model state set.')\n",
    "    optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "    print('Optimizer state set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab03efec-2d3b-483b-9500-043ed3e74192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c59ce4ab5f40428f978f0fdbb30ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67950 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 150]\n",
      "\n",
      "Epoch: 0 \tTraining Loss: 0.623871 \tValidation Loss: 0.524688\n",
      "Loss improved saving checkpoint... \n",
      "[Epoch 1 / 150]\n",
      "\n",
      "Epoch: 1 \tTraining Loss: 0.500304 \tValidation Loss: 0.493887\n",
      "Loss improved saving checkpoint... \n",
      "[Epoch 2 / 150]\n",
      "\n",
      "Epoch: 2 \tTraining Loss: 0.475877 \tValidation Loss: 0.485578\n",
      "Loss improved saving checkpoint... \n",
      "[Epoch 3 / 150]\n",
      "\n",
      "Epoch: 3 \tTraining Loss: 0.460090 \tValidation Loss: 0.482507\n",
      "Loss improved saving checkpoint... \n",
      "[Epoch 4 / 150]\n",
      "\n",
      "Epoch: 4 \tTraining Loss: 0.447354 \tValidation Loss: 0.480377\n",
      "Loss improved saving checkpoint... \n",
      "[Epoch 5 / 150]\n",
      "\n",
      "Epoch: 5 \tTraining Loss: 0.436885 \tValidation Loss: 0.482273\n",
      "[Epoch 6 / 150]\n",
      "\n",
      "Epoch: 6 \tTraining Loss: 0.427460 \tValidation Loss: 0.483772\n",
      "[Epoch 7 / 150]\n",
      "\n",
      "Epoch: 7 \tTraining Loss: 0.418930 \tValidation Loss: 0.485243\n",
      "[Epoch 8 / 150]\n",
      "\n",
      "Epoch: 8 \tTraining Loss: 0.412001 \tValidation Loss: 0.487656\n",
      "[Epoch 9 / 150]\n",
      "\n",
      "Epoch: 9 \tTraining Loss: 0.406038 \tValidation Loss: 0.488560\n",
      "\n",
      "Loss did not reduce for last 5 epochs. Stopped training..\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "early_stopping_counter = 0\n",
    "early_stopping_limit = params['early_stopping_limit']\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    train_loss=0\n",
    "    valid_loss =0\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f\"[Epoch {epoch} / {num_train_epochs}]\")\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        loss, out, actual = model(batch)\n",
    "        #loss = outputs.loss\n",
    "        loss.backward(loss)#, retain_graph = True)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss+= ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(loss = train_loss)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch_idx, batch in enumerate(val_dataloader):\n",
    "        with torch.no_grad():\n",
    "            loss, out, actual = model(batch)\n",
    "\n",
    "\n",
    "        labels = actual\n",
    "        predictions = out\n",
    "        \n",
    "        valid_loss+= ((1 / (batch_idx + 1)) * (loss.data.item() - valid_loss))\n",
    "        \n",
    "\n",
    "    print('\\nEpoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "    #early stopping, checkpointing\n",
    "    if valid_loss < best_valid_loss:\n",
    "        early_stopping_counter = 0\n",
    "        best_valid_loss = valid_loss\n",
    "        #create checkpoint\n",
    "        print(\"Loss improved saving checkpoint... \")\n",
    "        checkpoint_builder(model, optimizer, epoch, save_checkpoint_path)\n",
    "\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_limit:\n",
    "            print(f'\\nLoss did not reduce for last {early_stopping_counter} epochs. Stopped training..')\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be4efe2-b438-4e2e-a267-d56c89d41cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('envpy3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "da21ce15d6df5ad024774a392a1d8e145e78a5d2dfb1784f247b0a3f84306330"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
